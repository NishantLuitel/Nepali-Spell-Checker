{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e495f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'NepaliBERT'...\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/Rajan/NepaliBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a1da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.8/277.8 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "     -------------------------------------- 330.3/330.3 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: filelock in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tqdm>=4.27 in f:\\spellchecker\\venv\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\spellchecker\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\spellchecker\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: colorama in f:\\spellchecker\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\spellchecker\\venv\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\spellchecker\\venv\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\spellchecker\\venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\spellchecker\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc68055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "vocab_file_dir = './NepaliBERT/' \n",
    "tokenizer = BertTokenizer.from_pretrained(vocab_file_dir,\n",
    "                                        strip_accents=False,\n",
    "                                         clean_text=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d957ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained('./NepaliBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3fd66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784a2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc9bbd59",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SpellChecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSpellChecker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SpellChecker'"
     ]
    }
   ],
   "source": [
    "from SpellChecker import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c2df40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trie_depth.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorrector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m correct_current_word,extract_choices_word,autocorrect_word\n",
      "File \u001b[1;32mF:\\SpellChecker\\experiments\\..\\corrector.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m final_candidate_words\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "File \u001b[1;32mF:\\SpellChecker\\experiments\\..\\utils.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextdistance\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetaphone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m doublemetaphone\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrie_depth.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     trie_depth \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/depth_dict.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trie_depth.pickle'"
     ]
    }
   ],
   "source": [
    "from corrector import correct_current_word,extract_choices_word,autocorrect_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
