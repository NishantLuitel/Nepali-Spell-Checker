{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ea8655",
   "metadata": {},
   "source": [
    "# Use deBerta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def64a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f07420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to $y$tem path the parent directory\n",
    "import os\n",
    "import sys\n",
    "notebook_dir = os.path.abspath(os.path.dirname(''))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43503ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4c7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Sakonii/deberta-base-nepali')\n",
    "model = AutoModelForMaskedLM.from_pretrained('Sakonii/deberta-base-nepali')\n",
    "\n",
    "# prepare input\n",
    "text = \"चाहिएको text यता राख्नु होला।\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# forward pass\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d957ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7ff630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851e4b3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9abf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = ['हरेक [MASK] नेपामको संविधानक पालना गर्नुपर्छ ।' ,\n",
    "                    'म पुस्तकालयबाट थुलो किताब पढ्न चाहन्छ ।',\n",
    "                    'तर उस समयमा पनि स्वस्थ राजनैतिक वातावरनको अभावले गर्दा देश विकासतर्फ विशेष प्रगति हुन  सकेन।',\n",
    "                   'नेपालमा आधुनिक रुपमा आर्थक विकाससम्बन्धी कार्यरू प्रारम्भ भएको हालै मात्र हो।',\n",
    "                   'हार धुनुहोस् र स्वास्थ जीवन जिउनुहोस्।',\n",
    "                   'जब प्रवीधिहरू एकीकृत हुन सूरु गर्छन् अर्थतन्त्र तथ सँस्कृति पनि निश्चितरूपमा विस्तारै एकीकृत हुने छ।',\n",
    "                   'उद्देयहरुमा पनि कुनै एक उद्देश्य पूर्ति नहुँदै अर्को नयाँ  उद्देश्यको रुपमा लिइने परम्परा बस्यो।',\n",
    "                   'लगानीकर्ताहरूको धयान तुरुन्त फेरियो , व्यापारीहरुले वताए ।',\n",
    "                    'अति धेरै हिज्जे गलती भएका शब्दहरू । तपाईँले टाइप गर्दा हिज्जे जाँच अक्षम पारयो ।',\n",
    "                    'लामो',\n",
    "                    'म नेपाली राम्रोसँग बोल्दिन',\n",
    "                    'तपाईंको उमर कति हो?',\n",
    "                    'मलाइ एक्लै छोडनुहोस्',\n",
    "                   'रुसी राष्ट्रपती पुटिनको प्रेम जावन त्य्ती सफल छैन ।']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83703bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4ca56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 74, 4909, 17, 9193, 743, 2780, 2024, 5815, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(sample_sentences[1])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1a0952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'६१'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(9538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c1c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb13eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(sample_sentences,padding = 'max_length',max_length =max_length,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5855e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = t['input_ids']\n",
    "mask = t['attention_mask']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(ids)\n",
    "mask_ids = torch.tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27afe626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,   317,  3184,  2897,  2145,  2490,  4631,  2982, 11433,    33,\n",
       "             61,     6,   324,    44,  2863,  1192,     5,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    74,  4909,    17,  9193,   743,  2780,  2024,  5815,     5,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    47,  4541,   278,    15,  3858,  3706,    50,    84,  6745,\n",
       "             23,     6, 11638,   150,   207,    76,   373,   302,  1601,    67,\n",
       "          12005,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   211,  1306,    88,   326,  2858,    44,    76,   793,   210,\n",
       "           2899,  2190,    16,  2171,    90,    87,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  2711,  7588,  2676,     9, 10213,   368, 12776,  2676,    31,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   745,  7063,    34,  2350,    49,  2597,    67,  4660,  1073,\n",
       "            890,  2574,    60,   445, 13525,    15,  1394,  3925,  3649,  2597,\n",
       "             40,    41,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0, 14621,    30,   119,   608,    15,    75,    26,  1463, 10479,\n",
       "          15212,   258,    94,  1463,     6,    88, 11582,  1173, 11427,    31,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  4661,   180,  1281,  4249,  8674,   704,   382,    10,     7,\n",
       "          16717,    11, 12144,     5,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  1248,    85, 12387,  9683,  1361,    34,    73,  6600,     5,\n",
       "           4079, 11295,   150, 12387,  9683,  2687,   148,  1432,    61,  1521,\n",
       "            382,     5,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   402,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    74,    39,  5100,  2903,   108,  1335,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  1480,   580,    61,    18,   465, 10130,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  7007,  3759,  4225,    23,  2676,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  5718, 15173, 11525,     6,   752,  1430,  2151,    10,  2368,\n",
       "           5825,   349,   104,     5,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1]]),\n",
       " torch.Size([14, 64]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e071788a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc3d30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 74, 4909, 17, 9193, 743, 2780, 2024, 5815, 5, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sample_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f805bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model(input_ids.to(device),mask_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b17fa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  317,\n",
       "  3184,\n",
       "  2897,\n",
       "  2145,\n",
       "  2490,\n",
       "  4631,\n",
       "  2982,\n",
       "  11433,\n",
       "  33,\n",
       "  61,\n",
       "  6,\n",
       "  324,\n",
       "  44,\n",
       "  2863,\n",
       "  1192,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  74,\n",
       "  4909,\n",
       "  17,\n",
       "  9193,\n",
       "  743,\n",
       "  2780,\n",
       "  2024,\n",
       "  5815,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  47,\n",
       "  4541,\n",
       "  278,\n",
       "  15,\n",
       "  3858,\n",
       "  3706,\n",
       "  50,\n",
       "  84,\n",
       "  6745,\n",
       "  23,\n",
       "  6,\n",
       "  11638,\n",
       "  150,\n",
       "  207,\n",
       "  76,\n",
       "  373,\n",
       "  302,\n",
       "  1601,\n",
       "  67,\n",
       "  12005,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  211,\n",
       "  1306,\n",
       "  88,\n",
       "  326,\n",
       "  2858,\n",
       "  44,\n",
       "  76,\n",
       "  793,\n",
       "  210,\n",
       "  2899,\n",
       "  2190,\n",
       "  16,\n",
       "  2171,\n",
       "  90,\n",
       "  87,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  2711,\n",
       "  7588,\n",
       "  2676,\n",
       "  9,\n",
       "  10213,\n",
       "  368,\n",
       "  12776,\n",
       "  2676,\n",
       "  31,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  745,\n",
       "  7063,\n",
       "  34,\n",
       "  2350,\n",
       "  49,\n",
       "  2597,\n",
       "  67,\n",
       "  4660,\n",
       "  1073,\n",
       "  890,\n",
       "  2574,\n",
       "  60,\n",
       "  445,\n",
       "  13525,\n",
       "  15,\n",
       "  1394,\n",
       "  3925,\n",
       "  3649,\n",
       "  2597,\n",
       "  40,\n",
       "  41,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  14621,\n",
       "  30,\n",
       "  119,\n",
       "  608,\n",
       "  15,\n",
       "  75,\n",
       "  26,\n",
       "  1463,\n",
       "  10479,\n",
       "  15212,\n",
       "  258,\n",
       "  94,\n",
       "  1463,\n",
       "  6,\n",
       "  88,\n",
       "  11582,\n",
       "  1173,\n",
       "  11427,\n",
       "  31,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  4661,\n",
       "  180,\n",
       "  1281,\n",
       "  4249,\n",
       "  8674,\n",
       "  704,\n",
       "  382,\n",
       "  10,\n",
       "  7,\n",
       "  16717,\n",
       "  11,\n",
       "  12144,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1248,\n",
       "  85,\n",
       "  12387,\n",
       "  9683,\n",
       "  1361,\n",
       "  34,\n",
       "  73,\n",
       "  6600,\n",
       "  5,\n",
       "  4079,\n",
       "  11295,\n",
       "  150,\n",
       "  12387,\n",
       "  9683,\n",
       "  2687,\n",
       "  148,\n",
       "  1432,\n",
       "  61,\n",
       "  1521,\n",
       "  382,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  402,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  74,\n",
       "  39,\n",
       "  5100,\n",
       "  2903,\n",
       "  108,\n",
       "  1335,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1480,\n",
       "  580,\n",
       "  61,\n",
       "  18,\n",
       "  465,\n",
       "  10130,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  7007,\n",
       "  3759,\n",
       "  4225,\n",
       "  23,\n",
       "  2676,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  5718,\n",
       "  15173,\n",
       "  11525,\n",
       "  6,\n",
       "  752,\n",
       "  1430,\n",
       "  2151,\n",
       "  10,\n",
       "  2368,\n",
       "  5825,\n",
       "  349,\n",
       "  104,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b46c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.reshape(14*64,-1)[torch.arange(14*64),input_ids.reshape(14*64)].reshape(14,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efcd6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f[torch.arange(input_ids.shape[0])[:, None], torch.arange(input_ids.shape[1]), input_ids][:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c5f923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'यो पुस्तकालयबाट स्लो किताब पढ्न चाहन्छ । तरंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंं'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(ls(v.logits)[1],dim=1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1bc5b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 36, 91, 11, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('नेपालिले')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111e38a",
   "metadata": {},
   "source": [
    "# Functions to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a386618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = torch.nn.LogSoftmax(dim = 2)\n",
    "# f = ls(v.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef2b1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import BrillMoore\n",
    "import regex as re\n",
    "from utils import final_candidate_words, return_lexicon_dict\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "def words(text): \n",
    "    text = re.sub(r'[\\u0964]', r'\\u0020\\u0964\\u0020', text)\n",
    "    return re.findall(r'[\\u0900-\\u097F]+', text.lower())\n",
    "\n",
    "final_lexicon_dict = return_lexicon_dict()\n",
    "\n",
    "with open(os.path.join(notebook_dir, '..','models','bma_27dec.pickle'),'rb') as f:\n",
    "    bma = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e0f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_bm(sentence,candidate_sentence):\n",
    "    '''\n",
    "    Returns P(Possible Typo Sentence/Candidate Correct Sentence)\n",
    "    \n",
    "    Uses Naive approach to compute probability for sentence from individual words\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    prod = 1\n",
    "    for word,candidate_word in zip(sentence.split(),candidate_sentence):          \n",
    "        prod*= bma.likelihood(word,candidate_word)\n",
    "        #print(\"prod:\",prod)\n",
    "    return prod\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.65\n",
    "def constant_distributive_likelihood(sentence,candidate_sentence,candidate_count):\n",
    "    prod = 1    \n",
    "    i = 0\n",
    "    #print(sentence.split(),candidate_sentence)\n",
    "    \n",
    "    for word,candidate_word in zip(sentence.split(),candidate_sentence):        \n",
    "        if word==candidate_word:\n",
    "            prod*= alpha\n",
    "        else:\n",
    "            N = candidate_count[i]\n",
    "            prod*= (1-alpha)/N\n",
    "        i+=1\n",
    "    return prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517851ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b2ddb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]\n",
      "[40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78]\n",
      "[80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118]\n",
      "[120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158]\n",
      "[160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198]\n",
      "[200, 202, 204, 206, 208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232, 234, 236, 238]\n",
      "[240, 242, 244, 246, 248, 250, 252, 254, 256, 258, 260, 262, 264, 266, 268, 270, 272, 274, 276, 278]\n",
      "[280, 282, 284, 286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310, 312, 314, 316, 318]\n",
      "[320, 322, 324, 326, 328, 330, 332, 334, 336, 338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358]\n",
      "[360, 362, 364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388, 390, 392, 394, 396, 398]\n",
      "[400, 402, 404, 406, 408, 410, 412, 414, 416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438]\n",
      "[440, 442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466, 468, 470, 472, 474, 476, 478]\n",
      "[480, 482, 484, 486, 488, 490, 492, 494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518]\n",
      "[520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544, 546, 548, 550, 552, 554, 556, 558]\n",
      "[560, 562, 564, 566, 568, 570, 572, 574, 576, 578, 580, 582, 584, 586, 588, 590, 592, 594, 596, 598]\n",
      "[600, 602, 604, 606, 608, 610, 612, 614, 616, 618, 620, 622, 624, 626, 628, 630, 632, 634, 636, 638]\n",
      "[640, 642, 644, 646, 648, 650, 652, 654, 656, 658, 660, 662, 664, 666, 668, 670, 672, 674, 676, 678]\n",
      "[680, 682, 684, 686, 688, 690, 692, 694, 696, 698, 700, 702, 704, 706, 708, 710, 712, 714, 716, 718]\n",
      "[720, 722, 724, 726, 728, 730, 732, 734, 736, 738, 740, 742, 744, 746, 748, 750, 752, 754, 756, 758]\n",
      "[760, 762, 764, 766, 768, 770, 772, 774, 776, 778, 780, 782, 784, 786, 788, 790, 792, 794, 796, 798]\n",
      "[800, 802, 804, 806, 808, 810, 812, 814, 816, 818, 820, 822, 824, 826, 828, 830, 832, 834, 836, 838]\n",
      "[840, 842, 844, 846, 848, 850, 852, 854, 856, 858, 860, 862, 864, 866, 868, 870, 872, 874, 876, 878]\n",
      "[880, 882, 884, 886, 888, 890, 892, 894, 896, 898, 900, 902, 904, 906, 908, 910, 912, 914, 916, 918]\n",
      "[920, 922, 924, 926, 928, 930, 932, 934, 936, 938, 940, 942, 944, 946, 948, 950, 952, 954, 956, 958]\n",
      "[960, 962, 964, 966, 968, 970, 972, 974, 976, 978, 980, 982, 984, 986, 988, 990, 992, 994, 996, 998]\n",
      "[1000, 1002, 1004, 1006, 1008, 1010, 1012, 1014, 1016, 1018, 1020, 1022, 1024, 1026, 1028, 1030, 1032, 1034, 1036, 1038]\n",
      "[1040, 1042, 1044, 1046, 1048, 1050, 1052, 1054, 1056, 1058, 1060, 1062, 1064, 1066, 1068, 1070, 1072, 1074, 1076, 1078]\n",
      "[1080, 1082, 1084, 1086, 1088, 1090, 1092, 1094, 1096, 1098, 1100, 1102, 1104, 1106, 1108, 1110, 1112, 1114, 1116, 1118]\n",
      "[1120, 1122, 1124, 1126, 1128, 1130, 1132, 1134, 1136, 1138, 1140, 1142, 1144, 1146, 1148, 1150, 1152, 1154, 1156, 1158]\n",
      "[1160, 1162, 1164, 1166, 1168, 1170, 1172, 1174, 1176, 1178, 1180, 1182, 1184, 1186, 1188, 1190, 1192, 1194, 1196, 1198]\n",
      "[1200, 1202, 1204, 1206, 1208, 1210, 1212, 1214, 1216, 1218, 1220, 1222, 1224, 1226, 1228, 1230, 1232, 1234, 1236, 1238]\n",
      "[1240, 1242, 1244, 1246, 1248, 1250, 1252, 1254, 1256, 1258, 1260, 1262, 1264, 1266, 1268, 1270, 1272, 1274, 1276, 1278]\n",
      "[1280, 1282, 1284, 1286, 1288, 1290, 1292, 1294, 1296, 1298, 1300, 1302, 1304, 1306, 1308, 1310, 1312, 1314, 1316, 1318]\n",
      "[1320, 1322, 1324, 1326, 1328, 1330, 1332, 1334, 1336, 1338, 1340, 1342, 1344, 1346, 1348, 1350, 1352, 1354, 1356, 1358]\n",
      "[1360, 1362, 1364, 1366, 1368, 1370, 1372, 1374, 1376, 1378, 1380, 1382, 1384, 1386, 1388, 1390, 1392, 1394, 1396, 1398]\n",
      "[1400, 1402, 1404, 1406, 1408, 1410, 1412, 1414, 1416, 1418, 1420, 1422, 1424, 1426, 1428, 1430, 1432, 1434, 1436, 1438]\n",
      "[1440, 1442, 1444, 1446, 1448, 1450, 1452, 1454, 1456, 1458, 1460, 1462, 1464, 1466, 1468, 1470, 1472, 1474, 1476, 1478]\n",
      "[1480, 1482, 1484, 1486, 1488, 1490, 1492, 1494, 1496, 1498, 1500, 1502, 1504, 1506, 1508, 1510, 1512, 1514, 1516, 1518]\n",
      "[1520, 1522, 1524, 1526, 1528, 1530, 1532, 1534, 1536, 1538, 1540, 1542, 1544, 1546, 1548, 1550, 1552, 1554, 1556, 1558]\n",
      "[1560, 1562, 1564, 1566, 1568, 1570, 1572, 1574, 1576, 1578, 1580, 1582, 1584, 1586, 1588, 1590, 1592, 1594, 1596, 1598]\n",
      "[1600, 1602, 1604, 1606, 1608, 1610, 1612, 1614, 1616, 1618, 1620, 1622, 1624, 1626, 1628, 1630, 1632, 1634, 1636, 1638]\n",
      "[1640, 1642, 1644, 1646, 1648, 1650, 1652, 1654, 1656, 1658, 1660, 1662, 1664, 1666, 1668, 1670, 1672, 1674, 1676, 1678]\n",
      "[1680, 1682, 1684, 1686, 1688, 1690, 1692, 1694, 1696, 1698, 1700, 1702, 1704, 1706, 1708, 1710, 1712, 1714, 1716, 1718]\n",
      "[1720, 1722, 1724, 1726, 1728, 1730, 1732, 1734, 1736, 1738, 1740, 1742, 1744, 1746, 1748, 1750, 1752, 1754, 1756, 1758]\n",
      "[1760, 1762, 1764, 1766, 1768, 1770, 1772, 1774, 1776, 1778, 1780, 1782, 1784, 1786, 1788, 1790, 1792, 1794, 1796, 1798]\n",
      "[1800, 1802, 1804, 1806, 1808, 1810, 1812, 1814, 1816, 1818, 1820, 1822, 1824, 1826, 1828, 1830, 1832, 1834, 1836, 1838]\n",
      "[1840, 1842, 1844, 1846, 1848, 1850, 1852, 1854, 1856, 1858, 1860, 1862, 1864, 1866, 1868, 1870, 1872, 1874, 1876, 1878]\n",
      "[1880, 1882, 1884, 1886, 1888, 1890, 1892, 1894, 1896, 1898, 1900, 1902, 1904, 1906, 1908, 1910, 1912, 1914, 1916, 1918]\n",
      "[1920, 1922, 1924, 1926, 1928, 1930, 1932, 1934, 1936, 1938, 1940, 1942, 1944, 1946, 1948, 1950, 1952, 1954, 1956, 1958]\n",
      "[1960, 1962, 1964, 1966, 1968, 1970, 1972, 1974, 1976, 1978, 1980, 1982, 1984, 1986, 1988, 1990, 1992, 1994, 1996, 1998]\n",
      "[2000, 2002, 2004, 2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020, 2022, 2024, 2026, 2028, 2030, 2032, 2034, 2036, 2038]\n",
      "[2040, 2042, 2044, 2046, 2048, 2050, 2052, 2054, 2056, 2058, 2060, 2062, 2064, 2066, 2068, 2070, 2072, 2074, 2076, 2078]\n",
      "[2080, 2082, 2084, 2086, 2088, 2090, 2092, 2094, 2096, 2098, 2100, 2102, 2104, 2106, 2108, 2110, 2112, 2114, 2116, 2118]\n",
      "[2120, 2122, 2124, 2126, 2128, 2130, 2132, 2134, 2136, 2138, 2140, 2142, 2144, 2146, 2148, 2150, 2152, 2154, 2156, 2158]\n",
      "[2160, 2162, 2164, 2166, 2168, 2170, 2172, 2174, 2176, 2178, 2180, 2182, 2184, 2186, 2188, 2190, 2192, 2194, 2196, 2198]\n",
      "[2200, 2202, 2204, 2206, 2208, 2210, 2212, 2214, 2216, 2218, 2220, 2222, 2224, 2226, 2228, 2230, 2232, 2234, 2236, 2238]\n",
      "[2240, 2242, 2244, 2246, 2248, 2250, 2252, 2254, 2256, 2258, 2260, 2262, 2264, 2266, 2268, 2270, 2272, 2274, 2276, 2278]\n",
      "[2280, 2282, 2284, 2286, 2288, 2290, 2292, 2294, 2296, 2298, 2300, 2302, 2304, 2306, 2308, 2310, 2312, 2314, 2316, 2318]\n",
      "[2320, 2322, 2324, 2326, 2328, 2330, 2332, 2334, 2336, 2338, 2340, 2342, 2344, 2346, 2348, 2350, 2352, 2354, 2356, 2358]\n",
      "[2360, 2362, 2364, 2366, 2368, 2370, 2372, 2374, 2376, 2378, 2380, 2382, 2384, 2386, 2388, 2390, 2392, 2394, 2396, 2398]\n",
      "[2400, 2402, 2404, 2406, 2408, 2410, 2412, 2414, 2416, 2418, 2420, 2422, 2424, 2426, 2428, 2430, 2432, 2434, 2436, 2438]\n",
      "[2440, 2442, 2444, 2446, 2448, 2450, 2452, 2454, 2456, 2458, 2460, 2462, 2464, 2466, 2468, 2470, 2472, 2474, 2476, 2478]\n",
      "[2480, 2482, 2484, 2486, 2488, 2490, 2492, 2494, 2496, 2498]\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "k=[2*i for i in range(1250)]  \n",
    "t=[]\n",
    "for i in range(1250)[::20]:\n",
    "    print(k[i:i+20])\n",
    "    t+=k[i:i+20]\n",
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76db903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctize_entire_nn(sentence, model,p_lambda = 1,prior='transformer',trie = False,likelihood = 'default'):\n",
    "    \n",
    "    tokens = words(sentence)\n",
    "\n",
    "    candidates = []    \n",
    "    \n",
    "    #Forcing to limit the number of candidate sentences\n",
    "    for _ in tokens:\n",
    "        candidates.append(final_candidate_words(_,use_trie = trie,force = True))\n",
    "    \n",
    "    \n",
    "#     candidate_sentences = list(itertools.product(*candidates))[:]\n",
    "\n",
    "    cs = list(itertools.product(*candidates))    \n",
    "    candidate_sentences = [' '.join(sent) for sent in cs]\n",
    "    if prior == 'transformer':\n",
    "        \n",
    "        cpl = []\n",
    "        for i in range(len(candidate_sentences))[::100]:\n",
    "#             print(i)\n",
    "            t = tokenizer(candidate_sentences[i:i+100],padding = 'max_length',max_length =max_length,truncation=True)    \n",
    "            ids = t['input_ids']\n",
    "            mask = t['attention_mask']\n",
    "            input_ids = torch.tensor(ids)\n",
    "            mask_ids = torch.tensor(mask)\n",
    "            v = model(input_ids.to(device),mask_ids.to(device))\n",
    "            f = ls(v.logits)\n",
    "            c = f[torch.arange(input_ids.shape[0])[:, None], torch.arange(input_ids.shape[1]), input_ids][:, :, None].squeeze(2)        \n",
    "            candidate_probability = torch.sum((c*mask_ids.to(device))[:],dim = 1)\n",
    "            cpl+=candidate_probability.tolist()\n",
    "\n",
    "#         print(len(candidate_sentences),len(cpl))\n",
    "        if likelihood=='default':\n",
    "            candidate_count = [len(_) for _ in candidates]  \n",
    "            sentences_probab_post=[(row*p_lambda) +\n",
    "                                   math.log(constant_distributive_likelihood(sentence,candidate_sentence,candidate_count)) \n",
    "                                   for row,candidate_sentence in zip(cpl,cs)]\n",
    "        elif likelihood=='bm':\n",
    "            sentences_probab_post=[(row*p_lambda) + \n",
    "                                    math.log(likelihood_bm(sentence,candidate_sentence)) \n",
    "                                    for row,candidate_sentence in zip(cpl,cs)]\n",
    "            \n",
    "        sorted_index = torch.argsort(torch.tensor(sentences_probab_post))\n",
    "        sentences_probab_post_sorted = sorted(sentences_probab_post,reverse = True)\n",
    "        \n",
    "        return [candidate_sentences[int(k)].split() for k in torch.flip(sorted_index,dims=(0,))],sentences_probab_post_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a4d5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctize_with_window_nn(sentence,model,window = 3,p_lambda = 1,prior = 'transformer',trie = False,likelihood = 'default'):\n",
    "    '''\n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    tokens = words(sentence)\n",
    "    if len(tokens) <= window:\n",
    "#         print(correctize_entire_nn(sentence,model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood))\n",
    "        return [correctize_entire_nn(sentence,model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood)]\n",
    "    else:\n",
    "        windows = [tokens[n:window+n] for n in range(0,len(tokens),window-1) if window+n <len(tokens)-1]    \n",
    "        remaining = (window-1)*len(windows)\n",
    "        windows.append(tokens[remaining:])\n",
    "        corrects = []\n",
    "        for _ in windows:\n",
    "            d = correctize_entire_nn(' '.join(_),model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood)\n",
    "            corrects.append(d)\n",
    "#         print(corrects)\n",
    "        return corrects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a195b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_choices2(sample_sentences,model,p_lambda = 1,trie = False,model_type ='knlm' ,likelihood = 'default'):\n",
    "    \n",
    "    if model_type =='knlm':\n",
    "        d = correctize_with_window_knlm(sample_sentences,model,p_lambda =p_lambda,trie = trie,likelihood = likelihood)\n",
    "        window_candidates = []\n",
    "        window_probab = []\n",
    "        for window in d:\n",
    "            maxim = min(len(window[0]),10)\n",
    "            top_candidates = window[0][:maxim]\n",
    "            window_candidates.append(top_candidates)\n",
    "            window_probab.append(window[1][:maxim])\n",
    "        return window_candidates,window_probab\n",
    "    \n",
    "    if model_type == 'transformer':\n",
    "        d = correctize_with_window_nn(sample_sentences,model,p_lambda =p_lambda,trie = trie,likelihood = likelihood)\n",
    "        window_candidates = []\n",
    "        window_probab = []\n",
    "        for window in d:\n",
    "            maxim = min(len(window[0]),10)\n",
    "            top_candidates = window[0][:maxim]\n",
    "            window_candidates.append(top_candidates)\n",
    "            window_probab.append(window[1][:maxim])\n",
    "        return window_candidates,window_probab\n",
    "        \n",
    "def extract_choices(sample_sentences,model,p_lambda = 1,trie = False,likelihood = 'default',model_type = 'knlm'):\n",
    "    \n",
    "    \n",
    "    wc,wp = return_choices2(sample_sentences,model,p_lambda = p_lambda,trie = trie ,model_type = model_type,likelihood = likelihood)\n",
    "#     choices_list=[set() for i in range(len(sample_sentences.split())+1)]\n",
    "    choices_list=[[] for i in range(len(sample_sentences.split())+1)]\n",
    "#     print(len(choices_list))\n",
    "\n",
    "    const = 0\n",
    "    for _ in wc:\n",
    "        for sens in _:\n",
    "            for i,w in enumerate(sens):\n",
    "                index = i + const\n",
    "                if w not in choices_list[index]:\n",
    "                    choices_list[index].append(w)\n",
    "        const += len(wc[0][0])-1\n",
    "    if len(choices_list[len(choices_list)-1]) == 0:\n",
    "        return choices_list[:len(choices_list)-1]\n",
    "    return choices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3711de2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50\n",
      "125 125\n",
      "125 125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['म', 'मा', 'आम', 'अ', 'आ'],\n",
       " ['पुस्तकालयबाट', 'पुस्तकालयमा'],\n",
       " ['थुलो', 'थलो', 'ठुलो', 'ठूलो', 'ठुला'],\n",
       " ['किताब', 'किताबमा'],\n",
       " ['पढ्न', 'पढ्ने', 'बढ्न', 'चढ्न'],\n",
       " ['चाहन्छ', 'चाहन्छु', 'चाहन्छन्', 'चाहिन्छ'],\n",
       " ['।', 'आ']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_choices(sample_sentences[1],model=model,p_lambda = 0.6,trie = True,likelihood = 'bm',model_type = 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ab9fbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['हरेक [MASK] नेपामको संविधानक पालना गर्नुपर्छ ।',\n",
       " 'म पुस्तकालयबाट थुलो किताब पढ्न चाहन्छ ।',\n",
       " 'तर उस समयमा पनि स्वस्थ राजनैतिक वातावरनको अभावले गर्दा देश विकासतर्फ विशेष प्रगति हुन  सकेन।',\n",
       " 'नेपालमा आधुनिक रुपमा आर्थक विकाससम्बन्धी कार्यरू प्रारम्भ भएको हालै मात्र हो।',\n",
       " 'हार धुनुहोस् र स्वास्थ जीवन जिउनुहोस्।',\n",
       " 'जब प्रवीधिहरू एकीकृत हुन सूरु गर्छन् अर्थतन्त्र तथ सँस्कृति पनि निश्चितरूपमा विस्तारै एकीकृत हुने छ।',\n",
       " 'उद्देयहरुमा पनि कुनै एक उद्देश्य पूर्ति नहुँदै अर्को नयाँ  उद्देश्यको रुपमा लिइने परम्परा बस्यो।',\n",
       " 'लगानीकर्ताहरूको धयान तुरुन्त फेरियो , व्यापारीहरुले वताए ।',\n",
       " 'अति धेरै हिज्जे गलती भएका शब्दहरू । तपाईँले टाइप गर्दा हिज्जे जाँच अक्षम पारयो ।',\n",
       " 'लामो',\n",
       " 'म नेपाली राम्रोसँग बोल्दिन',\n",
       " 'तपाईंको उमर कति हो?',\n",
       " 'मलाइ एक्लै छोडनुहोस्',\n",
       " 'रुसी राष्ट्रपती पुटिनको प्रेम जावन त्य्ती सफल छैन ।']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4c61ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['तर'],\n",
       " ['उस'],\n",
       " ['समयमा'],\n",
       " ['पनि', 'पति', 'अनि'],\n",
       " ['स्वस्थ', 'स्वस्थ्य', 'ध्वस्त', 'अस्वस्थ'],\n",
       " ['राजनीति'],\n",
       " ['वातावरणको'],\n",
       " ['अभावले', 'अभावमा', 'सभाले'],\n",
       " ['गर्दा', 'गर्दै', 'गर्ला', 'गर्न', 'गर्व'],\n",
       " ['देश'],\n",
       " ['विकासतर्फ'],\n",
       " ['विशेष', 'बिशेष', 'विदेश'],\n",
       " ['प्रगति', 'प्रति', 'प्रालि', 'प्रगतिको', 'प्रगाढ'],\n",
       " ['हुन'],\n",
       " ['सकेन', 'सकिन', 'सकिएन'],\n",
       " ['।', 'आ', 'अ', 'ः', 'इ']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_choices(sample_sentences[2],model=model,p_lambda =0.135 ,trie = True,likelihood = 'bm',model_type = 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ece0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तर उस समयमा पनि स्वस्थ राजनैतिक वातावरनको अभावले गर्दा देश विकासतर्फ विशेष प्रगति हुन  सकेन।'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fd8f9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6877070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = torch.nn.LogSoftmax(dim = 2)\n",
    "max_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac3bbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import gather_dataset,WER,word_accuracy,char_accuracy\n",
    "\n",
    "dataset_file = os.path.join(notebook_dir, '..','data','eval_data2.pic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5160228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gather_dataset(dataset_file)[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_tokens = [words(t[0]) for t in dataset]\n",
    "error_tokens = [words(t[1]) for t in dataset]\n",
    "error_sentences = [t[1] for t in dataset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b05c5f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2e4aa6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0  :  (0.0, 0, 4) WER1:  0.3076923076923077 WER2:  0.3076923076923077\n",
      "2 0.0  :  (0.0, 0, 13) WER1:  0.3333333333333333 WER2:  0.3333333333333333\n",
      "4 0.0  :  (0.0, 0, 24) WER1:  0.34285714285714286 WER2:  0.34285714285714286\n",
      "6 0.0  :  (0.0, 0, 28) WER1:  0.28865979381443296 WER2:  0.28865979381443296\n",
      "8 0.0  :  (0.0, 0, 37) WER1:  0.2781954887218045 WER2:  0.2781954887218045\n",
      "0.0  :  (0.0, 0, 42) WER1:  0.2727272727272727 WER2:  0.2727272727272727\n",
      "0 0.2  :  (0.25, 1, 4) WER1:  0.3076923076923077 WER2:  0.23076923076923078\n",
      "2 0.2  :  (0.23076923076923078, 3, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 0.2  :  (0.16666666666666666, 4, 24) WER1:  0.34285714285714286 WER2:  0.32857142857142857\n",
      "6 0.2  :  (0.14285714285714285, 4, 28) WER1:  0.28865979381443296 WER2:  0.27835051546391754\n",
      "8 0.2  :  (0.1891891891891892, 7, 37) WER1:  0.2781954887218045 WER2:  0.2706766917293233\n",
      "0.2  :  (0.19047619047619047, 8, 42) WER1:  0.2727272727272727 WER2:  0.2597402597402597\n",
      "0 0.4  :  (0.5, 2, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 0.4  :  (0.3076923076923077, 4, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 0.4  :  (0.25, 6, 24) WER1:  0.34285714285714286 WER2:  0.3142857142857143\n",
      "6 0.4  :  (0.25, 7, 28) WER1:  0.28865979381443296 WER2:  0.25773195876288657\n",
      "8 0.4  :  (0.2972972972972973, 11, 37) WER1:  0.2781954887218045 WER2:  0.24812030075187969\n",
      "0.4  :  (0.30952380952380953, 13, 42) WER1:  0.2727272727272727 WER2:  0.24025974025974026\n",
      "0 0.6000000000000001  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.07692307692307693\n",
      "2 0.6000000000000001  :  (0.38461538461538464, 5, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 0.6000000000000001  :  (0.4166666666666667, 10, 24) WER1:  0.34285714285714286 WER2:  0.2857142857142857\n",
      "6 0.6000000000000001  :  (0.39285714285714285, 11, 28) WER1:  0.28865979381443296 WER2:  0.24742268041237114\n",
      "8 0.6000000000000001  :  (0.40540540540540543, 15, 37) WER1:  0.2781954887218045 WER2:  0.2556390977443609\n",
      "0.6000000000000001  :  (0.42857142857142855, 18, 42) WER1:  0.2727272727272727 WER2:  0.24025974025974026\n",
      "0 0.8  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 0.8  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 0.8  :  (0.4583333333333333, 11, 24) WER1:  0.34285714285714286 WER2:  0.2857142857142857\n",
      "6 0.8  :  (0.42857142857142855, 12, 28) WER1:  0.28865979381443296 WER2:  0.25773195876288657\n",
      "8 0.8  :  (0.43243243243243246, 16, 37) WER1:  0.2781954887218045 WER2:  0.2631578947368421\n",
      "0.8  :  (0.4523809523809524, 19, 42) WER1:  0.2727272727272727 WER2:  0.2532467532467532\n",
      "0 1.0  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 1.0  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 1.0  :  (0.4583333333333333, 11, 24) WER1:  0.34285714285714286 WER2:  0.2857142857142857\n",
      "6 1.0  :  (0.42857142857142855, 12, 28) WER1:  0.28865979381443296 WER2:  0.26804123711340205\n",
      "8 1.0  :  (0.4594594594594595, 17, 37) WER1:  0.2781954887218045 WER2:  0.2631578947368421\n",
      "1.0  :  (0.47619047619047616, 20, 42) WER1:  0.2727272727272727 WER2:  0.2532467532467532\n",
      "0 1.2000000000000002  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 1.2000000000000002  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 1.2000000000000002  :  (0.5, 12, 24) WER1:  0.34285714285714286 WER2:  0.3\n",
      "6 1.2000000000000002  :  (0.4642857142857143, 13, 28) WER1:  0.28865979381443296 WER2:  0.27835051546391754\n",
      "8 1.2000000000000002  :  (0.4864864864864865, 18, 37) WER1:  0.2781954887218045 WER2:  0.2706766917293233\n",
      "1.2000000000000002  :  (0.5238095238095238, 22, 42) WER1:  0.2727272727272727 WER2:  0.2662337662337662\n",
      "0 1.4000000000000001  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 1.4000000000000001  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3076923076923077\n",
      "4 1.4000000000000001  :  (0.5, 12, 24) WER1:  0.34285714285714286 WER2:  0.3\n",
      "6 1.4000000000000001  :  (0.4642857142857143, 13, 28) WER1:  0.28865979381443296 WER2:  0.27835051546391754\n",
      "8 1.4000000000000001  :  (0.4864864864864865, 18, 37) WER1:  0.2781954887218045 WER2:  0.2706766917293233\n",
      "1.4000000000000001  :  (0.5238095238095238, 22, 42) WER1:  0.2727272727272727 WER2:  0.2662337662337662\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    for j in range(8):\n",
    "        predicted_tokens = []\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =0.0+Gap*j,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i,Gap*j,' : ',word_accuracy(correct_tokens[:i+1],predicted_tokens[:i+1],error_tokens[:i+1]),'WER1: ',WER(correct_tokens[:i+1],error_tokens[:i+1] ),'WER2: ',WER(correct_tokens[:i+1],predicted_tokens[:i+1] ))\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "638aa333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5999999999999999  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 1.5999999999999999  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3333333333333333\n",
      "4 1.5999999999999999  :  (0.5, 12, 24) WER1:  0.34285714285714286 WER2:  0.3142857142857143\n",
      "6 1.5999999999999999  :  (0.4642857142857143, 13, 28) WER1:  0.28865979381443296 WER2:  0.28865979381443296\n",
      "8 1.5999999999999999  :  (0.4864864864864865, 18, 37) WER1:  0.2781954887218045 WER2:  0.2781954887218045\n",
      "0.2  :  (0.5238095238095238, 22, 42) WER1:  0.2727272727272727 WER2:  0.2857142857142857\n",
      "0 1.7999999999999998  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 1.7999999999999998  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.3333333333333333\n",
      "4 1.7999999999999998  :  (0.5, 12, 24) WER1:  0.34285714285714286 WER2:  0.3142857142857143\n",
      "6 1.7999999999999998  :  (0.4642857142857143, 13, 28) WER1:  0.28865979381443296 WER2:  0.28865979381443296\n",
      "8 1.7999999999999998  :  (0.4864864864864865, 18, 37) WER1:  0.2781954887218045 WER2:  0.2781954887218045\n",
      "0.4  :  (0.5238095238095238, 22, 42) WER1:  0.2727272727272727 WER2:  0.2857142857142857\n",
      "0 2.0  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 2.0  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.358974358974359\n",
      "4 2.0  :  (0.4583333333333333, 11, 24) WER1:  0.34285714285714286 WER2:  0.32857142857142857\n",
      "6 2.0  :  (0.42857142857142855, 12, 28) WER1:  0.28865979381443296 WER2:  0.29896907216494845\n",
      "8 2.0  :  (0.4594594594594595, 17, 37) WER1:  0.2781954887218045 WER2:  0.2932330827067669\n",
      "0.6000000000000001  :  (0.5, 21, 42) WER1:  0.2727272727272727 WER2:  0.3051948051948052\n",
      "0 2.2  :  (0.75, 3, 4) WER1:  0.3076923076923077 WER2:  0.15384615384615385\n",
      "2 2.2  :  (0.46153846153846156, 6, 13) WER1:  0.3333333333333333 WER2:  0.38461538461538464\n",
      "4 2.2  :  (0.4583333333333333, 11, 24) WER1:  0.34285714285714286 WER2:  0.34285714285714286\n",
      "6 2.2  :  (0.42857142857142855, 12, 28) WER1:  0.28865979381443296 WER2:  0.31958762886597936\n",
      "8 2.2  :  (0.4594594594594595, 17, 37) WER1:  0.2781954887218045 WER2:  0.3157894736842105\n",
      "0.8  :  (0.5, 21, 42) WER1:  0.2727272727272727 WER2:  0.3246753246753247\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    for j in range(1,5):\n",
    "        predicted_tokens = []\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =1.4+Gap*j,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i,1.4+Gap*j,' : ',word_accuracy(correct_tokens[:i+1],predicted_tokens[:i+1],error_tokens[:i+1]),'WER1: ',WER(correct_tokens[:i+1],error_tokens[:i+1] ),'WER2: ',WER(correct_tokens[:i+1],predicted_tokens[:i+1] ))\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb7e65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.6  :  (0.35714285714285715, 15, 42) WER1:  0.2727272727272727 WER2:  0.37012987012987014\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    predicted_tokens = []\n",
    "    for j in range(1):\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =1.4,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i)\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(0.6+Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29071eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3076923076923077 0.15384615384615385 (0.75, 3, 4) (0.75, 3, 4)\n",
      "2 0.3333333333333333 0.3076923076923077 (0.46153846153846156, 6, 13) (0.6428571428571429, 9, 14)\n",
      "4 0.34285714285714286 0.3 (0.5, 12, 24) (0.5555555555555556, 15, 27)\n",
      "6 0.28865979381443296 0.27835051546391754 (0.4642857142857143, 13, 28) (0.53125, 17, 32)\n",
      "8 0.2781954887218045 0.2706766917293233 (0.4864864864864865, 18, 37) (0.5348837209302325, 23, 43)\n",
      "10 0.25882352941176473 0.27058823529411763 (0.5227272727272727, 23, 44) (0.5384615384615384, 28, 52)\n",
      "12 0.2604166666666667 0.2708333333333333 (0.5, 25, 50) (0.5, 30, 60)\n",
      "14 0.2747747747747748 0.2927927927927928 (0.45901639344262296, 28, 61) (0.4583333333333333, 33, 72)\n",
      "16 0.2777777777777778 0.2976190476190476 (0.44285714285714284, 31, 70) (0.4457831325301205, 37, 83)\n",
      "18 0.2757352941176471 0.2977941176470588 (0.44, 33, 75) (0.43333333333333335, 39, 90)\n",
      "20 0.28716216216216217 0.30405405405405406 (0.43529411764705883, 37, 85) (0.43564356435643564, 44, 101)\n",
      "22 0.2857142857142857 0.3100303951367781 (0.425531914893617, 40, 94) (0.41964285714285715, 47, 112)\n",
      "24 0.28939828080229224 0.30945558739255014 (0.42574257425742573, 43, 101) (0.4166666666666667, 50, 120)\n",
      "26 0.2872340425531915 0.31117021276595747 (0.42592592592592593, 46, 108) (0.41732283464566927, 53, 127)\n",
      "28 0.2756892230576441 0.3032581453634085 (0.42727272727272725, 47, 110) (0.4186046511627907, 54, 129)\n",
      "30 0.2668269230769231 0.3004807692307692 (0.42342342342342343, 47, 111) (0.4153846153846154, 54, 130)\n",
      "32 0.2606741573033708 0.30337078651685395 (0.4224137931034483, 49, 116) (0.41605839416058393, 57, 137)\n",
      "34 0.26004228329809725 0.3023255813953488 (0.4146341463414634, 51, 123) (0.4041095890410959, 59, 146)\n",
      "36 0.25250501002004005 0.30060120240480964 (0.4126984126984127, 52, 126) (0.40939597315436244, 61, 149)\n",
      "38 0.2466281310211946 0.2947976878612717 (0.4140625, 53, 128) (0.40789473684210525, 62, 152)\n",
      "40 0.24677716390423574 0.285451197053407 (0.43283582089552236, 58, 134) (0.4240506329113924, 67, 158)\n",
      "42 0.23958333333333334 0.2795138888888889 (0.4420289855072464, 61, 138) (0.43209876543209874, 70, 162)\n",
      "44 0.23993288590604026 0.2802013422818792 (0.4405594405594406, 63, 143) (0.4260355029585799, 72, 169)\n",
      "46 0.24398073836276082 0.2808988764044944 (0.4407894736842105, 67, 152) (0.42777777777777776, 77, 180)\n",
      "48 0.2415902140672783 0.27675840978593275 (0.4430379746835443, 70, 158) (0.43010752688172044, 80, 186)\n",
      "50 0.24269005847953215 0.2733918128654971 (0.45180722891566266, 75, 166) (0.43434343434343436, 86, 198)\n",
      "52 0.24413793103448275 0.2689655172413793 (0.4689265536723164, 83, 177) (0.44976076555023925, 94, 209)\n",
      "54 0.25227568270481143 0.27308192457737324 (0.4587628865979381, 89, 194) (0.45374449339207046, 103, 227)\n",
      "56 0.2561576354679803 0.27216748768472904 (0.46153846153846156, 96, 208) (0.44715447154471544, 110, 246)\n",
      "58 0.25943396226415094 0.2783018867924528 (0.4590909090909091, 101, 220) (0.4576923076923077, 119, 260)\n",
      "60 0.25904977375565613 0.27601809954751133 (0.4672489082969432, 107, 229) (0.4612546125461255, 125, 271)\n",
      "62 0.2546245919477693 0.27421109902067464 (0.46153846153846156, 108, 234) (0.4548736462093863, 126, 277)\n",
      "64 0.2555205047318612 0.27129337539432175 (0.4732510288065844, 115, 243) (0.4634146341463415, 133, 287)\n",
      "66 0.2584963954685891 0.27394438722966014 (0.4701195219123506, 118, 251) (0.4594594594594595, 136, 296)\n",
      "68 0.26147704590818366 0.27644710578842313 (0.46564885496183206, 122, 262) (0.4577922077922078, 141, 308)\n",
      "70 0.26011560693641617 0.27552986512524086 (0.46296296296296297, 125, 270) (0.4559748427672956, 145, 318)\n",
      "72 0.2611524163568773 0.27695167286245354 (0.46619217081850534, 131, 281) (0.46060606060606063, 152, 330)\n",
      "74 0.26377597109304424 0.27822944896115626 (0.4657534246575342, 136, 292) (0.4619883040935672, 158, 342)\n",
      "76 0.2618213660245184 0.27933450087565675 (0.46488294314381273, 139, 299) (0.46131805157593125, 161, 349)\n",
      "78 0.2614991482112436 0.28534923339011925 (0.46254071661237783, 142, 307) (0.45938375350140054, 164, 357)\n",
      "80 0.2601828761429759 0.286783042394015 (0.46006389776357826, 144, 313) (0.465564738292011, 169, 363)\n",
      "82 0.2635408245755861 0.2910266774454325 (0.4601226993865031, 150, 326) (0.46842105263157896, 178, 380)\n",
      "84 0.2623723487824038 0.2906520031421838 (0.46107784431137727, 154, 334) (0.4704370179948586, 183, 389)\n",
      "86 0.25766871165644173 0.2937116564417178 (0.46130952380952384, 155, 336) (0.4719387755102041, 185, 392)\n",
      "88 0.2570789865871833 0.29731743666169896 (0.45507246376811594, 157, 345) (0.4666666666666667, 189, 405)\n",
      "90 0.2578754578754579 0.295970695970696 (0.45454545454545453, 160, 352) (0.4733009708737864, 195, 412)\n",
      "92 0.25841088045812455 0.2956335003579098 (0.45706371191135736, 165, 361) (0.47393364928909953, 200, 422)\n",
      "94 0.2604748603351955 0.29678770949720673 (0.4584450402144772, 171, 373) (0.47392290249433106, 209, 441)\n",
      "96 0.26326530612244897 0.29931972789115646 (0.45478036175710596, 176, 387) (0.4682713347921225, 214, 457)\n",
      "98 0.2619047619047619 0.29894179894179895 (0.45202020202020204, 179, 396) (0.4669509594882729, 219, 469)\n",
      "100 0.26151849448410125 0.29980532121998704 (0.4466501240694789, 180, 403) (0.46638655462184875, 222, 476)\n",
      "102 0.2577580747308423 0.29892336922102597 (0.44717444717444715, 182, 407) (0.4698544698544699, 226, 481)\n",
      "104 0.25838509316770186 0.2981366459627329 (0.44711538461538464, 186, 416) (0.4684317718940937, 230, 491)\n",
      "106 0.26180257510729615 0.30104230533415083 (0.43559718969555034, 186, 427) (0.4633663366336634, 234, 505)\n",
      "108 0.2628398791540785 0.30151057401812686 (0.43908045977011495, 191, 435) (0.4708171206225681, 242, 514)\n",
      "110 0.26444312090530075 0.3025610482430018 (0.4369369369369369, 194, 444) (0.4722753346080306, 247, 523)\n",
      "112 0.26721120186697783 0.30046674445740956 (0.4432314410480349, 203, 458) (0.47680890538033394, 257, 539)\n",
      "114 0.26689576174112256 0.30011454753722794 (0.44420600858369097, 207, 466) (0.4789762340036563, 262, 547)\n",
      "116 0.26617895329206526 0.30106921778277995 (0.4439746300211416, 210, 473) (0.47833935018050544, 265, 554)\n",
      "118 0.2634498058790904 0.2995008319467554 (0.4442105263157895, 211, 475) (0.4802158273381295, 267, 556)\n",
      "120 0.2642430819316332 0.30005425935973956 (0.4455852156057495, 217, 487) (0.4807017543859649, 274, 570)\n",
      "122 0.2642514651038892 0.2972828982418753 (0.45161290322580644, 224, 496) (0.4879310344827586, 283, 580)\n",
      "124 0.263903462749213 0.2953830010493179 (0.45129224652087474, 227, 503) (0.48722316865417375, 286, 587)\n",
      "126 0.2639175257731959 0.29690721649484536 (0.453125, 232, 512) (0.49331103678929766, 295, 598)\n",
      "128 0.2627848101265823 0.29265822784810125 (0.4605009633911368, 239, 519) (0.4991735537190083, 302, 605)\n",
      "130 0.26389584376564845 0.2914371557336004 (0.4629981024667932, 244, 527) (0.5024469820554649, 308, 613)\n",
      "132 0.26495304003954523 0.291646070192783 (0.46455223880597013, 249, 536) (0.507223113964687, 316, 623)\n",
      "134 0.2627489072365226 0.28994657600777074 (0.46395563770794823, 251, 541) (0.505564387917329, 318, 629)\n",
      "136 0.2633587786259542 0.2895992366412214 (0.46195652173913043, 255, 552) (0.5031152647975078, 323, 642)\n",
      "138 0.26189354686764016 0.287329251059821 (0.46223021582733814, 257, 556) (0.5046296296296297, 327, 648)\n",
      "140 0.26149558755225266 0.2875058058522991 (0.46003552397868563, 259, 563) (0.5060975609756098, 332, 656)\n",
      "142 0.26301369863013696 0.28812785388127854 (0.4600694444444444, 265, 576) (0.5044776119402985, 338, 670)\n",
      "144 0.2642440556303275 0.2857783759533423 (0.466893039049236, 275, 589) (0.5094890510948905, 349, 685)\n",
      "146 0.26146384479717816 0.28703703703703703 (0.4688026981450253, 278, 593) (0.5101449275362319, 352, 690)\n",
      "148 0.2610581092801388 0.2862098872506505 (0.473421926910299, 285, 602) (0.5135908440629471, 359, 699)\n",
      "150 0.26103729104157736 0.28546935276468066 (0.4729064039408867, 288, 609) (0.5127478753541076, 362, 706)\n",
      "152 0.2610712779417967 0.2846900042176297 (0.47334410339256866, 293, 619) (0.5188284518828452, 372, 717)\n",
      "154 0.2602910602910603 0.28565488565488567 (0.4728434504792332, 296, 626) (0.5172413793103449, 375, 725)\n",
      "156 0.25980392156862747 0.2855392156862745 (0.47327044025157233, 301, 636) (0.5156037991858887, 380, 737)\n",
      "158 0.2584677419354839 0.2862903225806452 (0.47425897035881437, 304, 641) (0.5175202156334232, 384, 742)\n",
      "160 0.25774424146147734 0.28832406671961874 (0.4699537750385208, 305, 649) (0.516, 387, 750)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 0.2591868647380766 0.28967943706020327 (0.4675716440422323, 310, 663) (0.5136897001303781, 394, 767)\n",
      "164 0.2580893682588598 0.288135593220339 (0.4671641791044776, 313, 670) (0.5135483870967742, 398, 775)\n",
      "166 0.2588369441277081 0.28924363359939187 (0.46402349486049926, 316, 681) (0.5088832487309645, 401, 788)\n",
      "168 0.25966228893058163 0.28930581613508444 (0.4653179190751445, 322, 692) (0.5105853051058531, 410, 803)\n",
      "170 0.2588888888888889 0.29 (0.463519313304721, 324, 699) (0.5092478421701603, 413, 811)\n",
      "172 0.2601536772777168 0.2912550311013538 (0.45850914205344584, 326, 711) (0.5042219541616405, 418, 829)\n",
      "174 0.2606191504679626 0.2933765298776098 (0.4544198895027624, 329, 724) (0.5029585798816568, 425, 845)\n",
      "176 0.2610085227272727 0.2950994318181818 (0.45034013605442175, 331, 735) (0.5029103608847497, 432, 859)\n",
      "178 0.26073187895847993 0.294862772695285 (0.44804318488529016, 332, 741) (0.5028901734104047, 435, 865)\n",
      "180 0.2604602510460251 0.29358437935843795 (0.4497991967871486, 336, 747) (0.5034403669724771, 439, 872)\n",
      "182 0.2604202549087151 0.2941784361005856 (0.44841269841269843, 339, 756) (0.5034013605442177, 444, 882)\n",
      "184 0.2598639455782313 0.29625850340136056 (0.443717277486911, 339, 764) (0.4983164983164983, 444, 891)\n",
      "186 0.2602832097100472 0.29669588671611596 (0.4430051813471503, 342, 772) (0.49944506104328523, 450, 901)\n",
      "188 0.2600866955651884 0.29709903301100365 (0.4461538461538462, 348, 780) (0.5016429353778752, 458, 913)\n",
      "190 0.2595899470899471 0.296957671957672 (0.445859872611465, 350, 785) (0.5032608695652174, 463, 920)\n",
      "192 0.25876187356698327 0.2970848345889289 (0.4468354430379747, 353, 790) (0.5043196544276458, 467, 926)\n",
      "194 0.2578555231616456 0.296728215095562 (0.4472361809045226, 356, 796) (0.5042918454935622, 470, 932)\n",
      "196 0.25816784112748237 0.29788597053171045 (0.445409429280397, 359, 806) (0.5026455026455027, 475, 945)\n",
      "198 0.25877886744701045 0.2973742486554888 (0.44621026894865523, 365, 818) (0.503125, 483, 960)\n",
      "200 0.25733000623830316 0.2966313162819713 (0.4448484848484848, 367, 825) (0.5020661157024794, 486, 968)\n",
      "202 0.2573189522342065 0.29676425269645607 (0.4467065868263473, 373, 835) (0.5056179775280899, 495, 979)\n",
      "204 0.25755263960939884 0.2960024412572475 (0.4466824644549763, 377, 844) (0.5075834175935288, 502, 989)\n",
      "206 0.25754830917874394 0.29589371980676327 (0.4466588511137163, 381, 853) (0.5075075075075075, 507, 999)\n",
      "208 0.25850746268656716 0.29402985074626864 (0.4515011547344111, 391, 866) (0.511307767944936, 520, 1017)\n",
      "210 0.2590076786769049 0.2932663910218547 (0.4526795895096921, 397, 877) (0.5116731517509727, 526, 1028)\n",
      "212 0.25892334698654185 0.29315389116442364 (0.45084745762711864, 399, 885) (0.5096339113680154, 529, 1038)\n",
      "214 0.2588303416328894 0.29328314997104804 (0.44966442953020136, 402, 894) (0.5085714285714286, 534, 1050)\n",
      "216 0.2599713055954089 0.2938307030129125 (0.45143487858719644, 409, 906) (0.507029053420806, 541, 1067)\n",
      "218 0.26009096077316657 0.2930642410460489 (0.4524590163934426, 414, 915) (0.5074349442379182, 546, 1076)\n",
      "220 0.259061534138803 0.29277887046923295 (0.4490238611713666, 414, 922) (0.5046040515653776, 548, 1086)\n",
      "222 0.2587646076794658 0.2921535893155259 (0.44946236559139785, 418, 930) (0.5050228310502283, 553, 1095)\n",
      "224 0.2575048196089232 0.29110437895896446 (0.4502673796791444, 421, 935) (0.5063636363636363, 557, 1100)\n",
      "226 0.2574931880108992 0.29182561307901905 (0.4497354497354497, 425, 945) (0.5072072072072072, 563, 1110)\n",
      "228 0.25688073394495414 0.291958985429034 (0.4474789915966387, 426, 952) (0.5035714285714286, 564, 1120)\n",
      "230 0.2568218298555377 0.292402354200107 (0.44583333333333336, 428, 960) (0.5013262599469496, 567, 1131)\n",
      "232 0.2568393094289509 0.29269588313413014 (0.44570837642192346, 431, 967) (0.5008771929824561, 571, 1140)\n",
      "234 0.2572406529752501 0.2930489731437599 (0.44626407369498466, 436, 977) (0.5004344048653345, 576, 1151)\n",
      "236 0.2567778936392075 0.2948383733055266 (0.4436548223350254, 437, 985) (0.4987057808455565, 578, 1159)\n",
      "238 0.2579395817195972 0.29615285308546346 (0.44144144144144143, 441, 999) (0.4957482993197279, 583, 1176)\n",
      "240 0.25856777493606137 0.29539641943734013 (0.4431256181998022, 448, 1011) (0.49706129303106633, 592, 1191)\n",
      "242 0.25836713995943206 0.2956389452332657 (0.44160942100098133, 450, 1019) (0.4975, 597, 1200)\n",
      "244 0.2585427135678392 0.29673366834170856 (0.4402332361516035, 453, 1029) (0.4962840627580512, 601, 1211)\n",
      "246 0.2593423019431988 0.29596412556053814 (0.44284341978866476, 461, 1041) (0.4987755102040816, 611, 1225)\n",
      "248 0.25832716506291636 0.29583024919812484 (0.44317096466093603, 464, 1047) (0.4983792544570502, 615, 1234)\n",
      "250 0.2578908735013457 0.29557132370932226 (0.44212523719165087, 466, 1054) (0.49717969379532634, 617, 1241)\n",
      "252 0.25848690591658585 0.29558680892337535 (0.4427767354596623, 472, 1066) (0.49682539682539684, 626, 1260)\n",
      "254 0.2600336457582312 0.29560201874549386 (0.444547134935305, 481, 1082) (0.49882903981264637, 639, 1281)\n",
      "256 0.2591093117408907 0.2953084067635151 (0.44485294117647056, 484, 1088) (0.49922360248447206, 643, 1288)\n",
      "258 0.2583845063769485 0.2954652810581011 (0.443327239488117, 485, 1094) (0.49768160741885625, 644, 1294)\n",
      "260 0.25778506204635915 0.296183563568251 (0.444141689373297, 489, 1101) (0.49884704073789393, 649, 1301)\n",
      "262 0.25846867749419955 0.2965197215777262 (0.44344703770197486, 494, 1114) (0.5, 657, 1314)\n",
      "264 0.2585018382352941 0.2971047794117647 (0.44177777777777777, 497, 1125) (0.49698795180722893, 660, 1328)\n",
      "266 0.25846783359854514 0.29734030461468514 (0.44327176781002636, 504, 1137) (0.49739000745712153, 667, 1341)\n",
      "268 0.2587996389891697 0.2976083032490975 (0.44463818657367044, 510, 1147) (0.4977810650887574, 673, 1352)\n",
      "270 0.2589105581708137 0.29679444070836136 (0.4458874458874459, 515, 1155) (0.4996326230712711, 680, 1361)\n",
      "272 0.25873192436040043 0.2976640711902113 (0.44368013757523644, 516, 1163) (0.4978102189781022, 682, 1370)\n",
      "274 0.2583351733274454 0.29785824685361006 (0.4444444444444444, 520, 1170) (0.49927431059506533, 688, 1378)\n",
      "276 0.2579868708971554 0.2978118161925602 (0.44359626802374896, 523, 1179) (0.49855907780979825, 692, 1388)\n",
      "278 0.2576020851433536 0.29821894005212857 (0.44435075885328834, 527, 1186) (0.4989247311827957, 696, 1395)\n",
      "280 0.25706275609230106 0.29825318093595 (0.4446308724832215, 530, 1192) (0.4989293361884368, 699, 1401)\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = []\n",
    "for i,s in enumerate(error_sentences):\n",
    "    c = extract_choices(s,model=model,p_lambda = 1.4,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "#     print(c)\n",
    "    c = [t[0] for t in c]\n",
    "    predicted_tokens.append(c)\n",
    "    if i%2 == 0:\n",
    "        a=len(predicted_tokens)\n",
    "        print(i,WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] ),word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]))\n",
    "    if i==323:\n",
    "        a=len(predicted_tokens)\n",
    "        print(WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] ),word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1219ca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25042111173498033, 0.3741343814336515)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = len(predicted_tokens)\n",
    "a = 324\n",
    "WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4d26f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.351270553064275, 470, 1338)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fed99b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4460659898477157, 703, 1576)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c437acc",
   "metadata": {},
   "source": [
    "### con$tant di$tributive likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb406fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.0  :  (0.23809523809523808, 10, 42) WER1:  0.2727272727272727 WER2:  0.3246753246753247\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.2  :  (0.2619047619047619, 11, 42) WER1:  0.2727272727272727 WER2:  0.36363636363636365\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.4  :  (0.2857142857142857, 12, 42) WER1:  0.2727272727272727 WER2:  0.36363636363636365\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.6000000000000001  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.38961038961038963\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.8  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.43506493506493504\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.0  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.43506493506493504\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.2000000000000002  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.461038961038961\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.4000000000000001  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.4675324675324675\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    \n",
    "    for j in range(2,10):\n",
    "        predicted_tokens = []\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =Gap*j,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i)\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45c9255e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "160\n",
      "162\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "172\n",
      "174\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "202\n",
      "204\n",
      "206\n",
      "208\n",
      "210\n",
      "212\n",
      "214\n",
      "216\n",
      "218\n",
      "220\n",
      "222\n",
      "224\n",
      "226\n",
      "228\n",
      "230\n",
      "232\n",
      "234\n",
      "236\n",
      "238\n",
      "240\n",
      "242\n",
      "244\n",
      "246\n",
      "248\n",
      "250\n",
      "252\n",
      "254\n",
      "256\n",
      "258\n",
      "260\n",
      "262\n",
      "264\n",
      "266\n",
      "268\n",
      "270\n",
      "272\n",
      "274\n",
      "276\n",
      "278\n",
      "280\n",
      "282\n",
      "284\n",
      "286\n",
      "288\n",
      "290\n",
      "292\n",
      "294\n",
      "296\n",
      "298\n",
      "300\n",
      "302\n",
      "304\n",
      "306\n",
      "308\n",
      "310\n",
      "312\n",
      "314\n",
      "316\n",
      "318\n",
      "320\n",
      "322\n",
      "0.25042111173498033 0.4624742653939734 (0.31763826606875933, 425, 1338) (0.46890862944162437, 739, 1576)\n",
      "324\n",
      "326\n",
      "328\n",
      "330\n",
      "332\n",
      "334\n",
      "336\n",
      "338\n",
      "340\n",
      "342\n",
      "344\n",
      "346\n",
      "348\n",
      "350\n",
      "352\n",
      "354\n",
      "356\n",
      "358\n",
      "360\n",
      "362\n",
      "364\n",
      "366\n",
      "368\n",
      "370\n",
      "372\n",
      "374\n",
      "376\n",
      "378\n",
      "380\n",
      "382\n",
      "384\n",
      "386\n",
      "388\n",
      "390\n",
      "392\n",
      "394\n",
      "396\n",
      "398\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = []\n",
    "for i,s in enumerate(error_sentences):\n",
    "    c = extract_choices(s,model=model,p_lambda = 0.6,trie = True,likelihood = 'default',model_type = 'transformer')\n",
    "#     print(c)\n",
    "    c = [t[0] for t in c]\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_tokens.append(c)\n",
    "    if i%2 == 0:\n",
    "        print(i)\n",
    "    if i==323:\n",
    "        l = len(predicted_tokens)\n",
    "        print(WER(correct_tokens[:l],error_tokens[:l] ),WER(correct_tokens[:l],predicted_tokens[:l] ),word_accuracy(correct_tokens[:l],predicted_tokens[:l],error_tokens[:l]),char_accuracy(correct_tokens[:l],predicted_tokens[:l],error_tokens[:l]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91dfce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25042111173498033, 0.4624742653939734)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = len(predicted_tokens)\n",
    "a = 324\n",
    "WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36d69602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31763826606875933, 425, 1338)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d360251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46890862944162437, 739, 1576)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dee35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
