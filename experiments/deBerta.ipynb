{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ea8655",
   "metadata": {},
   "source": [
    "# Use deBerta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def64a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f07420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to $y$tem path the parent directory\n",
    "import os\n",
    "import sys\n",
    "notebook_dir = os.path.abspath(os.path.dirname(''))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43503ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f4c7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Sakonii/deberta-base-nepali')\n",
    "model = AutoModelForMaskedLM.from_pretrained('Sakonii/deberta-base-nepali')\n",
    "\n",
    "# prepare input\n",
    "text = \"चाहिएको text यता राख्नु होला।\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# forward pass\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d957ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7ff630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851e4b3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9abf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = ['हरेक [MASK] नेपामको संविधानक पालना गर्नुपर्छ ।' ,\n",
    "                    'म पुस्तकालयबाट थुलो किताब पढ्न चाहन्छ ।',\n",
    "                    'तर उस समयमा पनि स्वस्थ राजनैतिक वातावरनको अभावले गर्दा देश विकासतर्फ विशेष प्रगति हुन  सकेन।',\n",
    "                   'नेपालमा आधुनिक रुपमा आर्थक विकाससम्बन्धी कार्यरू प्रारम्भ भएको हालै मात्र हो।',\n",
    "                   'हार धुनुहोस् र स्वास्थ जीवन जिउनुहोस्।',\n",
    "                   'जब प्रवीधिहरू एकीकृत हुन सूरु गर्छन् अर्थतन्त्र तथ सँस्कृति पनि निश्चितरूपमा विस्तारै एकीकृत हुने छ।',\n",
    "                   'उद्देयहरुमा पनि कुनै एक उद्देश्य पूर्ति नहुँदै अर्को नयाँ  उद्देश्यको रुपमा लिइने परम्परा बस्यो।',\n",
    "                   'लगानीकर्ताहरूको धयान तुरुन्त फेरियो , व्यापारीहरुले वताए ।',\n",
    "                    'अति धेरै हिज्जे गलती भएका शब्दहरू । तपाईँले टाइप गर्दा हिज्जे जाँच अक्षम पारयो ।',\n",
    "                    'लामो',\n",
    "                    'म नेपाली राम्रोसँग बोल्दिन',\n",
    "                    'तपाईंको उमर कति हो?',\n",
    "                    'मलाइ एक्लै छोडनुहोस्',\n",
    "                   'रुसी राष्ट्रपती पुटिनको प्रेम जावन त्य्ती सफल छैन ।']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83703bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4ca56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 74, 4909, 17, 9193, 743, 2780, 2024, 5815, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(sample_sentences[1])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1a0952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'६१'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(9538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c1c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb13eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(sample_sentences,padding = 'max_length',max_length =max_length,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5855e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = t['input_ids']\n",
    "mask = t['attention_mask']\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(ids)\n",
    "mask_ids = torch.tensor(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27afe626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0,   317,  3184,  2897,  2145,  2490,  4631,  2982, 11433,    33,\n",
       "             61,     6,   324,    44,  2863,  1192,     5,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    74,  4909,    17,  9193,   743,  2780,  2024,  5815,     5,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    47,  4541,   278,    15,  3858,  3706,    50,    84,  6745,\n",
       "             23,     6, 11638,   150,   207,    76,   373,   302,  1601,    67,\n",
       "          12005,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   211,  1306,    88,   326,  2858,    44,    76,   793,   210,\n",
       "           2899,  2190,    16,  2171,    90,    87,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  2711,  7588,  2676,     9, 10213,   368, 12776,  2676,    31,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   745,  7063,    34,  2350,    49,  2597,    67,  4660,  1073,\n",
       "            890,  2574,    60,   445, 13525,    15,  1394,  3925,  3649,  2597,\n",
       "             40,    41,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0, 14621,    30,   119,   608,    15,    75,    26,  1463, 10479,\n",
       "          15212,   258,    94,  1463,     6,    88, 11582,  1173, 11427,    31,\n",
       "              2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  4661,   180,  1281,  4249,  8674,   704,   382,    10,     7,\n",
       "          16717,    11, 12144,     5,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  1248,    85, 12387,  9683,  1361,    34,    73,  6600,     5,\n",
       "           4079, 11295,   150, 12387,  9683,  2687,   148,  1432,    61,  1521,\n",
       "            382,     5,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,   402,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,    74,    39,  5100,  2903,   108,  1335,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  1480,   580,    61,    18,   465, 10130,     2,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  7007,  3759,  4225,    23,  2676,     2,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1],\n",
       "         [    0,  5718, 15173, 11525,     6,   752,  1430,  2151,    10,  2368,\n",
       "           5825,   349,   104,     5,     2,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1]]),\n",
       " torch.Size([14, 64]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e071788a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc3d30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 74, 4909, 17, 9193, 743, 2780, 2024, 5815, 5, 2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sample_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f805bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model(input_ids.to(device),mask_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b17fa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  317,\n",
       "  3184,\n",
       "  2897,\n",
       "  2145,\n",
       "  2490,\n",
       "  4631,\n",
       "  2982,\n",
       "  11433,\n",
       "  33,\n",
       "  61,\n",
       "  6,\n",
       "  324,\n",
       "  44,\n",
       "  2863,\n",
       "  1192,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  74,\n",
       "  4909,\n",
       "  17,\n",
       "  9193,\n",
       "  743,\n",
       "  2780,\n",
       "  2024,\n",
       "  5815,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  47,\n",
       "  4541,\n",
       "  278,\n",
       "  15,\n",
       "  3858,\n",
       "  3706,\n",
       "  50,\n",
       "  84,\n",
       "  6745,\n",
       "  23,\n",
       "  6,\n",
       "  11638,\n",
       "  150,\n",
       "  207,\n",
       "  76,\n",
       "  373,\n",
       "  302,\n",
       "  1601,\n",
       "  67,\n",
       "  12005,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  211,\n",
       "  1306,\n",
       "  88,\n",
       "  326,\n",
       "  2858,\n",
       "  44,\n",
       "  76,\n",
       "  793,\n",
       "  210,\n",
       "  2899,\n",
       "  2190,\n",
       "  16,\n",
       "  2171,\n",
       "  90,\n",
       "  87,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  2711,\n",
       "  7588,\n",
       "  2676,\n",
       "  9,\n",
       "  10213,\n",
       "  368,\n",
       "  12776,\n",
       "  2676,\n",
       "  31,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  745,\n",
       "  7063,\n",
       "  34,\n",
       "  2350,\n",
       "  49,\n",
       "  2597,\n",
       "  67,\n",
       "  4660,\n",
       "  1073,\n",
       "  890,\n",
       "  2574,\n",
       "  60,\n",
       "  445,\n",
       "  13525,\n",
       "  15,\n",
       "  1394,\n",
       "  3925,\n",
       "  3649,\n",
       "  2597,\n",
       "  40,\n",
       "  41,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  14621,\n",
       "  30,\n",
       "  119,\n",
       "  608,\n",
       "  15,\n",
       "  75,\n",
       "  26,\n",
       "  1463,\n",
       "  10479,\n",
       "  15212,\n",
       "  258,\n",
       "  94,\n",
       "  1463,\n",
       "  6,\n",
       "  88,\n",
       "  11582,\n",
       "  1173,\n",
       "  11427,\n",
       "  31,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  4661,\n",
       "  180,\n",
       "  1281,\n",
       "  4249,\n",
       "  8674,\n",
       "  704,\n",
       "  382,\n",
       "  10,\n",
       "  7,\n",
       "  16717,\n",
       "  11,\n",
       "  12144,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1248,\n",
       "  85,\n",
       "  12387,\n",
       "  9683,\n",
       "  1361,\n",
       "  34,\n",
       "  73,\n",
       "  6600,\n",
       "  5,\n",
       "  4079,\n",
       "  11295,\n",
       "  150,\n",
       "  12387,\n",
       "  9683,\n",
       "  2687,\n",
       "  148,\n",
       "  1432,\n",
       "  61,\n",
       "  1521,\n",
       "  382,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  402,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  74,\n",
       "  39,\n",
       "  5100,\n",
       "  2903,\n",
       "  108,\n",
       "  1335,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1480,\n",
       "  580,\n",
       "  61,\n",
       "  18,\n",
       "  465,\n",
       "  10130,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  7007,\n",
       "  3759,\n",
       "  4225,\n",
       "  23,\n",
       "  2676,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  5718,\n",
       "  15173,\n",
       "  11525,\n",
       "  6,\n",
       "  752,\n",
       "  1430,\n",
       "  2151,\n",
       "  10,\n",
       "  2368,\n",
       "  5825,\n",
       "  349,\n",
       "  104,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17b46c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.reshape(14*64,-1)[torch.arange(14*64),input_ids.reshape(14*64)].reshape(14,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efcd6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f[torch.arange(input_ids.shape[0])[:, None], torch.arange(input_ids.shape[1]), input_ids][:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c5f923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'यो पुस्तकालयबाट स्लो किताब पढ्न चाहन्छ । तरंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंंं'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(ls(v.logits)[1],dim=1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1bc5b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 36, 91, 11, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('नेपालिले')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111e38a",
   "metadata": {},
   "source": [
    "# Functions to correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a386618",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = torch.nn.LogSoftmax(dim = 2)\n",
    "# f = ls(v.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2b1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import BrillMoore\n",
    "import regex as re\n",
    "from utils import final_candidate_words, return_lexicon_dict\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "def words(text): \n",
    "    text = re.sub(r'[\\u0964]', r'\\u0020\\u0964\\u0020', text)\n",
    "    return re.findall(r'[\\u0900-\\u097F]+', text.lower())\n",
    "\n",
    "final_lexicon_dict = return_lexicon_dict()\n",
    "\n",
    "with open(os.path.join(notebook_dir, '..','models','bma_27dec.pickle'),'rb') as f:\n",
    "    bma = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e0f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_bm(sentence,candidate_sentence):\n",
    "    '''\n",
    "    Returns P(Possible Typo Sentence/Candidate Correct Sentence)\n",
    "    \n",
    "    Uses Naive approach to compute probability for sentence from individual words\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    prod = 1\n",
    "    for word,candidate_word in zip(sentence.split(),candidate_sentence):          \n",
    "        prod*= bma.likelihood(word,candidate_word)\n",
    "        #print(\"prod:\",prod)\n",
    "    return prod\n",
    "\n",
    "\n",
    "\n",
    "alpha = 0.65\n",
    "def constant_distributive_likelihood(sentence,candidate_sentence,candidate_count):\n",
    "    prod = 1    \n",
    "    i = 0\n",
    "    #print(sentence.split(),candidate_sentence)\n",
    "    \n",
    "    for word,candidate_word in zip(sentence.split(),candidate_sentence):        \n",
    "        if word==candidate_word:\n",
    "            prod*= alpha\n",
    "        else:\n",
    "            N = candidate_count[i]\n",
    "            prod*= (1-alpha)/N\n",
    "        i+=1\n",
    "    return prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76db903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctize_entire_nn(sentence, model,p_lambda = 1,prior='transformer',trie = False,likelihood = 'default'):\n",
    "    \n",
    "    tokens = words(sentence)\n",
    "\n",
    "    candidates = []    \n",
    "    \n",
    "    #Forcing to limit the number of candidate sentences\n",
    "    for _ in tokens:\n",
    "        candidates.append(final_candidate_words(_,use_trie = trie,force = True))\n",
    "    \n",
    "    \n",
    "#     candidate_sentences = list(itertools.product(*candidates))[:]\n",
    "\n",
    "    cs = list(itertools.product(*candidates))    \n",
    "    candidate_sentences = [' '.join(sent) for sent in cs]\n",
    "    if prior == 'transformer':\n",
    "        t = tokenizer(sample_sentences,padding = 'max_length',max_length =max_length,truncation=True)\n",
    "    \n",
    "        ids = t['input_ids']\n",
    "        mask = t['attention_mask']\n",
    "        input_ids = torch.tensor(ids)\n",
    "\n",
    "        mask_ids = torch.tensor(mask)\n",
    "        v = model(input_ids.to(device),mask_ids.to(device))\n",
    "        f = ls(v.logits)\n",
    "        c = f[torch.arange(input_ids.shape[0])[:, None], torch.arange(input_ids.shape[1]), input_ids][:, :, None].squeeze(2)\n",
    "        \n",
    "        candidate_probabilities = torch.sum((c*mask_ids.to(device))[1:],dim = 1)\n",
    "\n",
    "        if likelihood=='default':\n",
    "            candidate_count = [len(_) for _ in candidates]  \n",
    "            sentences_probab_post=[(row*p_lambda) +\n",
    "                                   math.log(constant_distributive_likelihood(sentence,candidate_sentence,candidate_count)) \n",
    "                                   for row,candidate_sentence in zip(candidate_probabilities,cs)]\n",
    "        elif likelihood=='bm':\n",
    "            sentences_probab_post=[(row*p_lambda) + \n",
    "                                    math.log(likelihood_bm(sentence,candidate_sentence)) \n",
    "                                    for row,candidate_sentence in zip(candidate_probabilities,cs)]\n",
    "            \n",
    "        sorted_index = torch.argsort(torch.tensor(sentences_probab_post))\n",
    "        sentences_probab_post_sorted = sorted(sentences_probab_post,reverse = True)\n",
    "        \n",
    "        return [candidate_sentences[int(k)].split() for k in torch.flip(sorted_index,dims=(0,))],sentences_probab_post_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a4d5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctize_with_window_nn(sentence,model,window = 5,p_lambda = 1,prior = 'transformer',trie = False,likelihood = 'default'):\n",
    "    '''\n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    tokens = words(sentence)\n",
    "    if len(tokens) <= window:\n",
    "#         print(correctize_entire_nn(sentence,model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood))\n",
    "        return [correctize_entire_nn(sentence,model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood)]\n",
    "    else:\n",
    "        windows = [tokens[n:window+n] for n in range(0,len(tokens),window-1) if window+n <len(tokens)-1]    \n",
    "        remaining = (window-1)*len(windows)\n",
    "        windows.append(tokens[remaining:])\n",
    "        corrects = []\n",
    "        for _ in windows:\n",
    "            d = correctize_entire_nn(' '.join(_),model,p_lambda=p_lambda,prior = prior,trie = trie,likelihood = likelihood)\n",
    "            corrects.append(d)\n",
    "#         print(corrects)\n",
    "        return corrects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a195b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_choices2(sample_sentences,model,p_lambda = 1,trie = False,model_type ='knlm' ,likelihood = 'default'):\n",
    "    \n",
    "    if model_type =='knlm':\n",
    "        d = correctize_with_window_knlm(sample_sentences,model,p_lambda =p_lambda,trie = trie,likelihood = likelihood)\n",
    "        window_candidates = []\n",
    "        window_probab = []\n",
    "        for window in d:\n",
    "            maxim = min(len(window[0]),10)\n",
    "            top_candidates = window[0][:maxim]\n",
    "            window_candidates.append(top_candidates)\n",
    "            window_probab.append(window[1][:maxim])\n",
    "        return window_candidates,window_probab\n",
    "    \n",
    "    if model_type == 'transformer':\n",
    "        d = correctize_with_window_nn(sample_sentences,model,p_lambda =p_lambda,trie = trie,likelihood = likelihood)\n",
    "        window_candidates = []\n",
    "        window_probab = []\n",
    "        for window in d:\n",
    "            maxim = min(len(window[0]),10)\n",
    "            top_candidates = window[0][:maxim]\n",
    "            window_candidates.append(top_candidates)\n",
    "            window_probab.append(window[1][:maxim])\n",
    "        return window_candidates,window_probab\n",
    "        \n",
    "def extract_choices(sample_sentences,model,p_lambda = 1,trie = False,likelihood = 'default',model_type = 'knlm'):\n",
    "    \n",
    "    \n",
    "    wc,wp = return_choices2(sample_sentences,model,p_lambda = p_lambda,trie = trie ,model_type = model_type,likelihood = likelihood)\n",
    "#     choices_list=[set() for i in range(len(sample_sentences.split())+1)]\n",
    "    choices_list=[[] for i in range(len(sample_sentences.split())+1)]\n",
    "#     print(len(choices_list))\n",
    "\n",
    "    const = 0\n",
    "    for _ in wc:\n",
    "        for sens in _:\n",
    "            for i,w in enumerate(sens):\n",
    "                index = i + const\n",
    "                if w not in choices_list[index]:\n",
    "                    choices_list[index].append(w)\n",
    "        const += len(wc[0][0])-1\n",
    "    if len(choices_list[len(choices_list)-1]) == 0:\n",
    "        return choices_list[:len(choices_list)-1]\n",
    "    return choices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3711de2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['म'],\n",
       " ['पुस्तकालयमा'],\n",
       " ['ठुलो'],\n",
       " ['किताब', 'कतार', 'किटान'],\n",
       " ['पढ्न', 'बढ्न', 'पढ्ने', 'चढ्न', 'पढ्दा'],\n",
       " ['चाहन्छ', 'चाहन्छु', 'चाहन्छन्'],\n",
       " ['।', 'आ', 'अ', 'ः', 'इ']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_choices(sample_sentences[1],model=model,p_lambda = 0.135,trie = True,likelihood = 'bm',model_type = 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4c61ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['तर'],\n",
       " ['उस'],\n",
       " ['समयमा'],\n",
       " ['पनि', 'पति', 'अनि'],\n",
       " ['स्वस्थ', 'स्वस्थ्य', 'ध्वस्त', 'अस्वस्थ'],\n",
       " ['राजनीति'],\n",
       " ['वातावरणको'],\n",
       " ['अभावले', 'अभावमा', 'सभाले'],\n",
       " ['गर्दा', 'गर्दै', 'गर्ला', 'गर्न', 'गर्व'],\n",
       " ['देश'],\n",
       " ['विकासतर्फ'],\n",
       " ['विशेष', 'बिशेष', 'विदेश'],\n",
       " ['प्रगति', 'प्रति', 'प्रालि', 'प्रगतिको', 'प्रगाढ'],\n",
       " ['हुन'],\n",
       " ['सकेन', 'सकिन', 'सकिएन'],\n",
       " ['।', 'आ', 'अ', 'ः', 'इ']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_choices(sample_sentences[2],model=model,p_lambda =0.135 ,trie = True,likelihood = 'bm',model_type = 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0ece0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'तर उस समयमा पनि स्वस्थ राजनैतिक वातावरनको अभावले गर्दा देश विकासतर्फ विशेष प्रगति हुन  सकेन।'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fd8f9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6877070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = torch.nn.LogSoftmax(dim = 2)\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac3bbdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import gather_dataset,WER,word_accuracy,char_accuracy\n",
    "\n",
    "dataset_file = os.path.join(notebook_dir, '..','data','eval_data2.pic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5160228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gather_dataset(dataset_file)[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_tokens = [words(t[0]) for t in dataset]\n",
    "error_tokens = [words(t[1]) for t in dataset]\n",
    "error_sentences = [t[1] for t in dataset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e4aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.6  :  (0.35714285714285715, 15, 42) WER1:  0.2727272727272727 WER2:  0.37012987012987014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    predicted_tokens = []\n",
    "    for j in range(1):\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =0.5,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i)\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(0.6+Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb7e65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.6  :  (0.35714285714285715, 15, 42) WER1:  0.2727272727272727 WER2:  0.37012987012987014\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    predicted_tokens = []\n",
    "    for j in range(1):\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =0.5,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i)\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(0.6+Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29071eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "160\n",
      "162\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "172\n",
      "174\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "202\n",
      "204\n",
      "206\n",
      "208\n",
      "210\n",
      "212\n",
      "214\n",
      "216\n",
      "218\n",
      "220\n",
      "222\n",
      "224\n",
      "226\n",
      "228\n",
      "230\n",
      "232\n",
      "234\n",
      "236\n",
      "238\n",
      "240\n",
      "242\n",
      "244\n",
      "246\n",
      "248\n",
      "250\n",
      "252\n",
      "254\n",
      "256\n",
      "258\n",
      "260\n",
      "262\n",
      "264\n",
      "266\n",
      "268\n",
      "270\n",
      "272\n",
      "274\n",
      "276\n",
      "278\n",
      "280\n",
      "282\n",
      "284\n",
      "286\n",
      "288\n",
      "290\n",
      "292\n",
      "294\n",
      "296\n",
      "298\n",
      "300\n",
      "302\n",
      "304\n",
      "306\n",
      "308\n",
      "310\n",
      "312\n",
      "314\n",
      "316\n",
      "318\n",
      "320\n",
      "322\n",
      "0.2727272727272727 0.38961038961038963 (0.38095238095238093, 16, 42) (0.4583333333333333, 22, 48)\n",
      "324\n",
      "326\n",
      "328\n",
      "330\n",
      "332\n",
      "334\n",
      "336\n",
      "338\n",
      "340\n",
      "342\n",
      "344\n",
      "346\n",
      "348\n",
      "350\n",
      "352\n",
      "354\n",
      "356\n",
      "358\n",
      "360\n",
      "362\n",
      "364\n",
      "366\n",
      "368\n",
      "370\n",
      "372\n",
      "374\n",
      "376\n",
      "378\n",
      "380\n",
      "382\n",
      "384\n",
      "386\n",
      "388\n",
      "390\n",
      "392\n",
      "394\n",
      "396\n",
      "398\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = []\n",
    "for i,s in enumerate(error_sentences):\n",
    "    c = extract_choices(s,model=model,p_lambda = 0.6,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "#     print(c)\n",
    "    c = [t[0] for t in c]\n",
    "    predicted_tokens.append(c)\n",
    "    if i%2 == 0:\n",
    "        print(i)\n",
    "    if i==323:\n",
    "        print(WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] ),word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1219ca40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25042111173498033, 0.3741343814336515)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = len(predicted_tokens)\n",
    "a = 324\n",
    "WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4d26f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.351270553064275, 470, 1338)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fed99b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4460659898477157, 703, 1576)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c437acc",
   "metadata": {},
   "source": [
    "### con$tant di$tributive likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb406fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.0  :  (0.23809523809523808, 10, 42) WER1:  0.2727272727272727 WER2:  0.3246753246753247\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.2  :  (0.2619047619047619, 11, 42) WER1:  0.2727272727272727 WER2:  0.36363636363636365\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.4  :  (0.2857142857142857, 12, 42) WER1:  0.2727272727272727 WER2:  0.36363636363636365\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.6000000000000001  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.38961038961038963\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0.8  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.43506493506493504\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.0  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.43506493506493504\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.2000000000000002  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.461038961038961\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "1.4000000000000001  :  (0.38095238095238093, 16, 42) WER1:  0.2727272727272727 WER2:  0.4675324675324675\n"
     ]
    }
   ],
   "source": [
    "Gap = 0.2\n",
    "a = 10\n",
    "def find_p_lambda(error_sentences):\n",
    "    \n",
    "    for j in range(8):\n",
    "        predicted_tokens = []\n",
    "        for i,s in enumerate(error_sentences[:a]):\n",
    "            c = extract_choices(s,model=model,p_lambda =Gap*j,trie = True,likelihood = 'bm',model_type = 'transformer')\n",
    "        #     print(c)\n",
    "            c = [t[0] for t in c]\n",
    "            predicted_tokens.append(c)\n",
    "            if i%2 == 0:\n",
    "                print(i)\n",
    "#         word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])\n",
    "        print(Gap*j,' : ',word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a]),'WER1: ',WER(correct_tokens[:a],error_tokens[:a] ),'WER2: ',WER(correct_tokens[:a],predicted_tokens[:a] ))\n",
    "find_p_lambda(error_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45c9255e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "30\n",
      "32\n",
      "34\n",
      "36\n",
      "38\n",
      "40\n",
      "42\n",
      "44\n",
      "46\n",
      "48\n",
      "50\n",
      "52\n",
      "54\n",
      "56\n",
      "58\n",
      "60\n",
      "62\n",
      "64\n",
      "66\n",
      "68\n",
      "70\n",
      "72\n",
      "74\n",
      "76\n",
      "78\n",
      "80\n",
      "82\n",
      "84\n",
      "86\n",
      "88\n",
      "90\n",
      "92\n",
      "94\n",
      "96\n",
      "98\n",
      "100\n",
      "102\n",
      "104\n",
      "106\n",
      "108\n",
      "110\n",
      "112\n",
      "114\n",
      "116\n",
      "118\n",
      "120\n",
      "122\n",
      "124\n",
      "126\n",
      "128\n",
      "130\n",
      "132\n",
      "134\n",
      "136\n",
      "138\n",
      "140\n",
      "142\n",
      "144\n",
      "146\n",
      "148\n",
      "150\n",
      "152\n",
      "154\n",
      "156\n",
      "158\n",
      "160\n",
      "162\n",
      "164\n",
      "166\n",
      "168\n",
      "170\n",
      "172\n",
      "174\n",
      "176\n",
      "178\n",
      "180\n",
      "182\n",
      "184\n",
      "186\n",
      "188\n",
      "190\n",
      "192\n",
      "194\n",
      "196\n",
      "198\n",
      "200\n",
      "202\n",
      "204\n",
      "206\n",
      "208\n",
      "210\n",
      "212\n",
      "214\n",
      "216\n",
      "218\n",
      "220\n",
      "222\n",
      "224\n",
      "226\n",
      "228\n",
      "230\n",
      "232\n",
      "234\n",
      "236\n",
      "238\n",
      "240\n",
      "242\n",
      "244\n",
      "246\n",
      "248\n",
      "250\n",
      "252\n",
      "254\n",
      "256\n",
      "258\n",
      "260\n",
      "262\n",
      "264\n",
      "266\n",
      "268\n",
      "270\n",
      "272\n",
      "274\n",
      "276\n",
      "278\n",
      "280\n",
      "282\n",
      "284\n",
      "286\n",
      "288\n",
      "290\n",
      "292\n",
      "294\n",
      "296\n",
      "298\n",
      "300\n",
      "302\n",
      "304\n",
      "306\n",
      "308\n",
      "310\n",
      "312\n",
      "314\n",
      "316\n",
      "318\n",
      "320\n",
      "322\n",
      "0.25042111173498033 0.4624742653939734 (0.31763826606875933, 425, 1338) (0.46890862944162437, 739, 1576)\n",
      "324\n",
      "326\n",
      "328\n",
      "330\n",
      "332\n",
      "334\n",
      "336\n",
      "338\n",
      "340\n",
      "342\n",
      "344\n",
      "346\n",
      "348\n",
      "350\n",
      "352\n",
      "354\n",
      "356\n",
      "358\n",
      "360\n",
      "362\n",
      "364\n",
      "366\n",
      "368\n",
      "370\n",
      "372\n",
      "374\n",
      "376\n",
      "378\n",
      "380\n",
      "382\n",
      "384\n",
      "386\n",
      "388\n",
      "390\n",
      "392\n",
      "394\n",
      "396\n",
      "398\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = []\n",
    "for i,s in enumerate(error_sentences):\n",
    "    c = extract_choices(s,model=model,p_lambda = 0.6,trie = True,likelihood = 'default',model_type = 'transformer')\n",
    "#     print(c)\n",
    "    c = [t[0] for t in c]\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_tokens.append(c)\n",
    "    if i%2 == 0:\n",
    "        print(i)\n",
    "    if i==323:\n",
    "        l = len(predicted_tokens)\n",
    "        print(WER(correct_tokens[:l],error_tokens[:l] ),WER(correct_tokens[:l],predicted_tokens[:l] ),word_accuracy(correct_tokens[:l],predicted_tokens[:l],error_tokens[:l]),char_accuracy(correct_tokens[:l],predicted_tokens[:l],error_tokens[:l]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91dfce0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25042111173498033, 0.4624742653939734)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = len(predicted_tokens)\n",
    "a = 324\n",
    "WER(correct_tokens[:a],error_tokens[:a] ),WER(correct_tokens[:a],predicted_tokens[:a] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36d69602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31763826606875933, 425, 1338)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d360251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46890862944162437, 739, 1576)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_accuracy(correct_tokens[:a],predicted_tokens[:a],error_tokens[:a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dee35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
